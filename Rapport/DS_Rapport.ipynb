{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Image class from PIL package  \n",
    "from PIL import Image  \n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ER Diagram \n",
    "\n",
    "![ER Diagram](https://i.imgur.com/naRsMrw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have created and imported our schema to our python notebook as an image. We have made the decision to exclude 'meta_keyword', 'type, 'domain', 'authors' and 'tag' as \n",
    "attributes of 'article', and instead have their own tables. This was done, to create the necessary entity-relations. For example, in the table 'type' there are 11 types, that each has their own id because 'type' is a one-to-many relation, and that id\n",
    "is saved in the 'article' instance. 'tag' on the other hand is a many-to-many relation, and we have therefore created a relation table that saves the tags associated with an article. In this \n",
    "relation table the id of the article and the tag is saved together. The other one-to-many and many-to-many relations works as described above.",
    "The SQL-code that was used to create the database can be found in the file \"SQL_database.sql\". \n",
    "\n",
    "The code used to separate the database and clean the data can be seen in the files 'Organize.ipynb' and 'Clean.ipynb' respectively. Here 'Clean.ipynb' cleans the content of the \n",
    "articles, and 'Organize.ipynb' separates the articles into different csv-files which we populate our database with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"detminkode\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"postgres\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(991691,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"Select Count(id)\n",
    "from article\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve task 3.1, we have created two queries, one with and one without INNER JOIN. We save our 'scraped_at' dates as 'Dates', to utilize the >= operator to find the articles that\n",
    "has been scraped after 15. Jan 2018. We also isolated the articles with type_id = 4, as this is the id for the articles classified as 'reliable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT domain_name.domain_name FROM domain_name\n",
    "WHERE domain_id in (SELECT article.domain_id FROM article\n",
    "WHERE article.type_id = '4' and scraped_at >= '2018-01-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without INNER JOIN above, and without it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT DISTINCT domain_name.domain_name FROM domain_name\n",
    "INNER JOIN article\n",
    "ON article.domain_id = domain_name.domain_id\n",
    "WHERE article.type_id = '4' and scraped_at >= '2018-01-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('christianpost.com',), ('consortiumnews.com',), ('nutritionfacts.org',)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT DISTINCT domain_name.domain_name FROM domain_name\n",
    "INNER JOIN article\n",
    "ON article.domain_id = domain_name.domain_id\n",
    "WHERE article.type_id = '12' and scraped_at >= '2018-01-15'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query below we find the author(s) with the most articles classified as fake news. This is done by finding all the authors for the articels that have been classified as fake, then count the authors, and lastly group all the authors that have the same number of fake articels. This is to make sure that if two or more authors have shared the most artikels. Then we order them and take first element which will be the largest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT array_agg(authorname), counter\n",
    "FROM (SELECT COUNT(author_in.authorid) as counter, author.authorname\n",
    "    FROM author_in\n",
    "    INNER JOIN author\n",
    "    ON author.authorid = author_in.authorid\n",
    "    WHERE Author.authorname != 'NoAuthor' AND author_in.id in (SELECT article.id FROM article WHERE article.type_id = 7)\n",
    "    GROUP BY author_in.authorid, author.authorname) AS authors\n",
    "GROUP BY counter\n",
    "ORDER BY counter DESC\n",
    "limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['John Rolls'], 1142)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT array_agg(authorname), counter\n",
    "FROM (SELECT COUNT(author_in.authorid) as counter, author.authorname\n",
    "    FROM author_in\n",
    "    INNER JOIN author\n",
    "    ON author.authorid = author_in.authorid\n",
    "    WHERE Author.authorname != 'NoAuthor' AND author_in.id in (SELECT article.id FROM article WHERE article.type_id = 7)\n",
    "    GROUP BY author_in.authorid, author.authorname) AS authors\n",
    "GROUP BY counter\n",
    "ORDER BY counter DESC\n",
    "limit 1 \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we used to count the pairs, was to compare 3 articles (a, b, c) and if they had the same meta_tags, to then create the pairs (a, b) and (a, c).\n",
    "We do not include the pair (b, c), as this information can be deduced by the other pairs. Therefore the number of pairs are Count(*) - 1, as can be seen \n",
    "on line 2 in the query. The way it works, is by collapsing all the meta_tags with the articles, so the meta_tags get saved as a list instead of all the \n",
    "different rows of data. We do this with the aggregate function 'array_agg'. After this we collapse the article id's that has the same list of meta_tags\n",
    "using the same 'array_agg' fuction, and if the length of the list is more than one then there are one or more pairs. Lastly, we then sum all the pairs,\n",
    "which returns the total number of article pairs with the same meta_tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT sum(pairAmount)\n",
    "FROM (SELECT count(*) - 1 as pairAmount\n",
    "\tFROM (SELECT id, array_agg(meta_keywords.meta_keyword) AS Meta_Keyword_ids\n",
    "\t\t\tFROM meta_keywords_in\n",
    "\t\t\tINNER JOIN meta_keywords\n",
    "\t\t\tON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "\t\t\tWHERE meta_keywords.meta_keyword_id != '0'\n",
    "\t\t\tGROUP BY meta_keywords_in.id\n",
    "\t\t ) meta_keywords_in\n",
    "\tGROUP BY Meta_Keyword_ids\n",
    "\tHAVING count(*) > 1) AS counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Decimal('24773'),)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(pairAmount)\n",
    "FROM (SELECT count(*) - 1 as pairAmount\n",
    "\tFROM (SELECT id, array_agg(meta_keywords.meta_keyword) AS Meta_Keyword_ids\n",
    "\t\t\tFROM meta_keywords_in\n",
    "\t\t\tINNER JOIN meta_keywords\n",
    "\t\t\tON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "\t\t\tWHERE meta_keywords.meta_keyword_id != '0'\n",
    "\t\t\tGROUP BY meta_keywords_in.id\n",
    "\t\t ) meta_keywords_in\n",
    "\tGROUP BY Meta_Keyword_ids\n",
    "\tHAVING count(*) > 1) AS counter\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1510, 'satire'),\n",
       " (1629, 'rumor'),\n",
       " (1914, 'unreliable'),\n",
       " (2399, 'conspiracy'),\n",
       " (2459, None),\n",
       " (2460, 'clickbait'),\n",
       " (3060, 'junksci'),\n",
       " (3072, 'fake'),\n",
       " (3501, 'political'),\n",
       " (3543, 'unknown'),\n",
       " (3667, 'bias'),\n",
       " (4365, 'reliable'),\n",
       " (8426, 'hate')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(length(content))/COUNT(id) as AvgLen, type\n",
    "from article\n",
    "Inner join type\n",
    "ON article.type_id = type.type_id\n",
    "GROUP BY type\n",
    "ORDER BY AvgLen ASC \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(123512, None),\n",
       " (5, 'texas'),\n",
       " (3, 'donald trump'),\n",
       " (3, 'Kfc'),\n",
       " (2, 'Clinton Foundation'),\n",
       " (2, 'president obama'),\n",
       " (2, 'secession'),\n",
       " (2, 'spontaneous combustion'),\n",
       " (2, 'trump'),\n",
       " (2, 'hillary clinton'),\n",
       " (2, 'Kentucky Fried Chicken'),\n",
       " (2, 'fox news'),\n",
       " (2, 'fried chicken')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT COUNT(meta_keywords_in.meta_keyword_id), meta_keywords.meta_keyword\n",
    "FROM meta_keywords_in\n",
    "inner join\n",
    "article\n",
    "ON article.id = meta_keywords_in.id and article.type_id = 7\n",
    "inner join \n",
    "meta_keywords\n",
    "ON meta_keywords.meta_keyword_id = meta_keywords_in.meta_keyword_id\n",
    "GROUP BY meta_keywords.meta_keyword\n",
    "Having COUNT(meta_keywords_in.meta_keyword_id) > 1\n",
    "ORDER BY count DESC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataexploration Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(43, 'rumor'),\n",
       " (18, None),\n",
       " (17, 'bias'),\n",
       " (17, 'hate'),\n",
       " (16, 'fake'),\n",
       " (16, 'political'),\n",
       " (16, 'conspiracy'),\n",
       " (16, 'unknown'),\n",
       " (16, 'clickbait'),\n",
       " (15, 'unreliable'),\n",
       " (14, 'junksci'),\n",
       " (12, 'satire'),\n",
       " (12, 'reliable')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execQuery(\"\"\"SELECT sum(length(tags.tag)) / count(*) as Avg, type.type\n",
    "FROM tags_in\n",
    "INNER JOIN article\n",
    "ON article.id = tags_in.id\n",
    "inner join tags\n",
    "ON tags.tag_id = tags_in.tag_id\n",
    "inner join type\n",
    "ON type.type_id = article.type_id\n",
    "GROUP BY type.type\n",
    "ORDER BY Avg DESC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I this task we had to make a web crawler that could scrape information from \"Politics and Conflict\" on Wikinews and based on our group number we would should only select a subset of all articles. This was given to us in the form of some python code to generate a string with the beginning letter for articles within our subset of articles.\n",
    "\n",
    "The first step we took was to figure out what kind of task we were given before we decided on a given tool.\n",
    "\n",
    "The first few observations we made were regarding how the webpage indexed its articles such that we could make our crawling logic.\n",
    "\n",
    "The first observation we made was that each \"entry point\" for a given letter was easy to get as it was just https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=[letter], where the letter was at the end of the url. on a given entry point there are links to categories and articles however there are a maximum 200 pages, even if there are more articles with a given starting letter, so we had to follow links to find all articles. A sticking point arose when we looked at \"indexing\"-urls after our entry points as they had no information of what letter/entry point we were coming from as they where of the form https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&pagefrom=[start article]+[end article], such that the url only described by article tiles what other articles where on that indexing page. This mean that our tool had to be able to follow links and have knowledge of what page it was coming from as it could not just use information on a page as well as jumping to all articles on an indexing page.\n",
    "\n",
    "Next we looked at a few articles and they seemed to have a general structure such that locating them on a page within HTML would be somewhat doable so we postponed the actual scraping part for later.\n",
    "\n",
    "Because of the requirements based on our initial assesment of the problem and our look at python web scraping tools we choose Scrapy as it is a feature rich tool made for making web crawlers and as we had some requirements that where non trivial ie. traversal logic more complicated then get all links and so on we choose it.\n",
    "\n",
    "When implementing our scraper, we encountered a lot obstacles along the way, amplified by the fact that we assigned tasks such that people with less explerience in an given subject had to do it for our assignment.\n",
    "\n",
    "We started by reading documentation while watching and reading tutorials as we building a dummy spider for a smaller part of the problem to get the basics of Scrapy right as well as understading HTML-markup node navigation. We made use of xpath to locate nodes within HTML. A snapshot of our Scrapy class we made is given below along with its output:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 8) <- shape of wikinews-dataset(row, col)\n",
      "(3680,) <- num of unique articles - seems to have the same number of rows no duplicates\n",
      "437 <- num of nan entries out of -> 29003 \n",
      "nan rate of 0.01506740681998414 %\n",
      "216 nan elements from sources_url column\n",
      "210 nan elements from about_sources_wiki_url column\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3680 entries, 0 to 3679\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   article_url             3680 non-null   object\n",
      " 1   categories              3680 non-null   object\n",
      " 2   content                 3678 non-null   object\n",
      " 3   publish_date            3671 non-null   object\n",
      " 4   scraped_at              3680 non-null   object\n",
      " 5   sources_url             3464 non-null   object\n",
      " 6   about_sources_wiki_url  3470 non-null   object\n",
      " 7   title                   3680 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 230.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filepath = '/home/daniel/OneDrive/KUuni/DataScience/Python/DS_5/wiki_news_nr_12.csv'\n",
    "\n",
    "# we read in the file\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# print shape -< how many rows and elements it has\n",
    "print(df.shape, \"<- shape of wikinews-dataset(row, col)\")\n",
    "\n",
    "# how many unique urls did we get(optimally as many as the rows in our dataframe)\n",
    "unique_articles = df['article_url'].unique().shape\n",
    "print(unique_articles, \"<- num of unique articles - seems to have the same number of rows no duplicates\")\n",
    "\n",
    "# how many fields without daata ie. nan out of all fiels\n",
    "df_nan_elms = df.isna().sum().sum()\n",
    "df_not_nan_elms = df.notna().sum().sum()\n",
    "print( df_nan_elms, \"<- num of nan entries out of ->\",  df_not_nan_elms ,\n",
    "     \"\\nnan rate of\", df_nan_elms/df_not_nan_elms, \"%\")\n",
    "# majority comes form articles not having sources or source wiki pages\n",
    "print(df['sources_url'].isna().sum(), \"nan elements from sources_url column\")\n",
    "print( df['about_sources_wiki_url'].isna().sum(), \"nan elements from about_sources_wiki_url column\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test spider below was used to see of we could iterate over a list of article-URLs if we could locate them on a navigation page. This early spider only has 2 parts; one for iterating over URLs and the other for getting article scraping data. The spider above worked fine, but the next step being creating the navigation logic became the greatest challenge, caused by a simple thing. We wrote the spider logic for another website as it was simpler and then adapted it to wikinews, however when we interchanged our other website with wikinews no files where generated when we selected to get output. After debugging we came across an Error about robot.txt which tunrns out is used when accesing websites from non-intruductionary-tutorials as it is a policy obayed by all never Scrapy spider by default if a website dosen't allow certain kinds of scrapers. After this we changed our spider to not obey the robot.txt but read https://en.wikipedia.org/robots.txt instead, and implimented some restrictions on the spider. Below the test spider we have included some of the settings we enabled to scrape more responsibly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` Test spider - bash\n",
    "class testSpider(scrapy.Spider):\n",
    "    name = \"test\"\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://en.wikinews.org/wiki/A_policeman_is_killed_and_another_one_is_tortured_in_MST_camp,_in_Brazil',\n",
    "\n",
    "            'https://en.wikinews.org/wiki/African_Union_refuses_to_arrest_Sudan%27s_President_for_war_crimes',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # get article content\n",
    "    def parse(self, response):\n",
    "        for info in response.xpath('//div[@id=\"content\"]'):\n",
    "            yield {\n",
    "                'title': info.xpath('//*[@id=\"firstHeading\"]/text()').get(),\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "``` Settings - python\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "\n",
    "# The initial download delay\n",
    "AUTOTHROTTLE_START_DELAY = 5\n",
    "\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "AUTOTHROTTLE_MAX_DELAY = 30\n",
    "\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "\n",
    "AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "HTTPCACHE_ENABLED = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final spider became somewhat complicated for a first spider and crawls without any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` python\n",
    "import string\n",
    "import re\n",
    "import scrapy\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, Compose\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from ..items import articleItem # location of item - used for scraped data structure\n",
    "\n",
    "# creating urls for chars based on group _nr - change group_nr to generate start_urls\n",
    "group_nr = 12 # <- change to get correct article set\n",
    "urls =[]\n",
    "for char in \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr % 23:group_nr % 23+10]:\n",
    "    urls.append('https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=' + char)\n",
    "print(*urls, sep='\\n')\n",
    "\n",
    "# main spider\n",
    "class wikiSpider(scrapy.Spider):\n",
    "    name = \"wiki\"\n",
    "\n",
    "    # start urls for scraping\n",
    "    def start_requests(self):\n",
    "\n",
    "        # urls used to spawn spider-instances\n",
    "        global urls\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # Set the maximum depth\n",
    "    maxdepth = 10\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\" Main method that parse downloaded pages. \"\"\"\n",
    "        # Set defaults for the first page that won't have any meta information\n",
    "        start_url = ''\n",
    "        from_url = ''\n",
    "        from_text = ''\n",
    "        depth = 0\n",
    "        # Extract the meta information from the response, if any\n",
    "        if 'start'  in response.meta: start_url = response.meta['start']\n",
    "        if 'from'   in response.meta: from_url  = response.meta['from']\n",
    "        if 'text'   in response.meta: from_text = response.meta['text']\n",
    "        if 'depth'  in response.meta: depth     = response.meta['depth']\n",
    "        \n",
    "        # set start url for crawler\n",
    "        if depth == 0:\n",
    "            start_url = response.url\n",
    "\n",
    "        # get all article links\n",
    "        if start_url[-1] == response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get(): # chek that current letter is on page\n",
    "\n",
    "            # change xpath to: ('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li[1]/a/@href') <- 1   page\n",
    "            # change xpath to: ('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li/a/@href')    <- 200 pages\n",
    "            articles = response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/ul/li/a/@href').getall()\n",
    "            for a in articles:\n",
    "                url = urljoin(response.url, a)\n",
    "                yield scrapy.Request(url, callback=self.parse_article)\n",
    "\n",
    "        ### DEBUG printing - used for locating spider behavior ###\n",
    "        print(\"### DEBUG DUMP STEP:\", depth, response.url, '<-', from_url, from_text, \"END ###\",\n",
    "              \"### DEBUG DUMP start_url:\", start_url[-1], response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get(),\"char_page END ###\")\n",
    "\n",
    "        # get nex_page only if maximum depth has not be reached and current char is still on page\n",
    "        if depth < self.maxdepth and start_url[-1] == response.xpath('//div[@id=\"mw-pages\"]/div/div/div[1]/h3/text()').get():\n",
    "            next_page = response.xpath('//div[@id=\"mw-pages\"]/a[2]') # location of next link\n",
    "            next_page_text = next_page.xpath(\"text()\").get()\n",
    "            next_page_link = next_page.xpath(\"@href\").get()\n",
    "            print(\"### DEBUG DUMP next_page:\", next_page, \"END ###\")\n",
    "\n",
    "            if next_page_link is not None:\n",
    "                request = response.follow(next_page_link, callback=self.parse)\n",
    "                # Meta information: URL of the current page\n",
    "                request.meta['from'] = response.url\n",
    "                # Meta information: text of the link\n",
    "                request.meta['text'] = next_page_text\n",
    "                # Meta information: depth of the link\n",
    "                request.meta['depth'] = depth + 1\n",
    "                # Meta information: start page for current crawler\n",
    "                request.meta['start'] = start_url\n",
    "                yield request\n",
    "\n",
    "    # get article content - using scrapy itemLoader and Items\n",
    "    def parse_article(self, response):\n",
    "        l = ItemLoader(item=articleItem(), response=response) # create itemloader l - following is adding to Fields\n",
    "        l.add_xpath('title',        '//*[@id=\"firstHeading\"]/text()')\n",
    "        l.add_xpath('publish_date', '//div[@id=\"catlinks\"]/div[@id=\"mw-normal-catlinks\"]/ul/li/a/text()',re='(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)[\\s,]*(?:\\d{1,2})[\\s,]*(?:\\d{4})')\n",
    "        l.add_xpath('content',      '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/p/text()|//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/p/child::a/text()|//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/text()', Join(' '))\n",
    "        l.add_xpath('categories',   '//div[@id=\"catlinks\"]/div[@id=\"mw-normal-catlinks\"]/ul/li/a/text()')\n",
    "        l.add_xpath('sources_url',  '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/span/a/@href')\n",
    "        l.add_xpath('about_sources_wiki_url', '//div[@id=\"mw-content-text\"]/div[@class=\"mw-parser-output\"]/ul/li/span/i/span/a/@href')\n",
    "        l.add_value('article_url', response.request.url)\n",
    "        l.add_value('scraped_at', (datetime.today().strftime('%Y-%m-%d')) )\n",
    "        yield l.load_item() # could use return/yield - no idea what changesw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea for the final Spider is that we generate based on group_nr our entry-point websites and generate a list of all article urls to follow afterwards we follow all these links we follow a link to the next 200 link-page and start again. By using metadata parameters we can inform the spider about where it has been where it is going and how deep it has gone. The last part about the spider to talk about is the function 'parse_article' where we make use of scrapy's item containers which help us deal with missing data in the case of a broken link or other unforeseen circumstances.\n",
    "\n",
    "The datafields we ended up collecting were:\n",
    "\n",
    "'article_url'\n",
    "\n",
    "'title' = title of the article inside the page\n",
    "\n",
    "'categories' = categories assigned to the article\n",
    "\n",
    "'content' = main text of article\n",
    "\n",
    "'publish_date'\n",
    "\n",
    "'scraped_at' = date of scraping by our spider\n",
    "\n",
    "'sources_url' = urls for all individual pages used as a source\n",
    "\n",
    "'about_sources_wiki_url' = url to wikipage about a given source ie. BBC\n",
    "\n",
    "\n",
    "We felt that these would be of use for further analyses for another group as well as being general enough that most articles would have an entry for all fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
